{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b93b4e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "import numpy as np\n",
    "\n",
    "from data_utils import (\n",
    "    get_embeddings, \n",
    "    synthesize_database, \n",
    "    synthesize_simple_database,\n",
    "    get_passenger_database,\n",
    "    check_passenger_exist,\n",
    "    create_simple_trainset\n",
    ")\n",
    "\n",
    "is_simple_data=True\n",
    "# Airport dataset\n",
    "embed_original, indices = get_embeddings() # Considered the query pictures\n",
    "if is_simple_data:\n",
    "    embed_data, id_data, location_data = synthesize_simple_database(embed_original)\n",
    "    date_data = None\n",
    "else:\n",
    "    embed_data, id_data, date_data, location_data = synthesize_database(embed_original)\n",
    "\n",
    "embed_data = np.stack(embed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7e59b7a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 128])\n",
      "torch.Size([1000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "x_train = np.array(embed_data[:1000, :])\n",
    "y_train = np.array(location_data[:1000])\n",
    "\n",
    "x_train = torch.FloatTensor(x_train)\n",
    "print(x_train.shape)\n",
    "\n",
    "y_train[y_train==2] = 0 # Make sure the class index start from 0 (0,1 in this example)\n",
    "y_train = torch.LongTensor(y_train)\n",
    "print(y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "348989a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gumbel_softmax(logits: torch.Tensor, tau: float = 1, hard: bool = False, eps: float = 1e-10, dim: int = -1) -> torch.Tensor:\n",
    "    r\"\"\"\n",
    "    Args:\n",
    "      logits: `[..., num_features]` unnormalized log probabilities\n",
    "      tau: non-negative scalar temperature\n",
    "      hard: if ``True``, the returned samples will be discretized as one-hot vectors,\n",
    "            but will be differentiated as if it is the soft sample in autograd\n",
    "      dim (int): A dimension along which softmax will be computed. Default: -1.\n",
    "\n",
    "    Returns:\n",
    "      Sampled tensor of same shape as `logits` from the Gumbel-Softmax distribution.\n",
    "      If ``hard=True``, the returned samples will be one-hot, otherwise they will\n",
    "      be probability distributions that sum to 1 across `dim`.\n",
    "    \"\"\"\n",
    "    gumbels = (\n",
    "        -torch.empty_like(logits, memory_format=torch.legacy_contiguous_format).exponential_().log()\n",
    "    )  # ~Gumbel(0,1)\n",
    "\n",
    "    gumbels = (logits + gumbels) / tau  # ~Gumbel(logits,tau)\n",
    "    y_soft = gumbels.softmax(dim)\n",
    "\n",
    "    if hard:\n",
    "        # Straight through.\n",
    "        index = y_soft.max(dim, keepdim=True)[1]\n",
    "        y_hard = torch.zeros_like(logits, memory_format=torch.legacy_contiguous_format).scatter_(dim, index, 1.0)\n",
    "        ret = y_hard - y_soft.detach() + y_soft\n",
    "    else:\n",
    "        # Reparametrization trick.\n",
    "        ret = y_soft\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f53aad31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_channel_mask(hidden_size_choices):\n",
    "    max_hidden_size = max(hidden_size_choices)\n",
    "    num_choices = len(hidden_size_choices)\n",
    "    masks = torch.zeros(max_hidden_size, num_choices)\n",
    "    for i in range(num_choices):\n",
    "        masks[:hidden_size_choices[i], i]=1\n",
    "    return masks\n",
    "    \n",
    "def get_flops_choices(input_size, hidden_size_choices, num_classes):\n",
    "    flops = []\n",
    "    for hidden_size in hidden_size_choices:\n",
    "        flops.append(2*hidden_size*input_size + 2*hidden_size*num_classes)\n",
    "    flops = np.array(flops)\n",
    "    return flops\n",
    "    \n",
    "class SuperNet_CS_LS(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size_choices, layer_choices, num_classes):\n",
    "        super(SuperNet_CS_LS, self).__init__()\n",
    "        \n",
    "        max_hidden_size = max(hidden_size_choices)\n",
    "        num_choices_hidden = len(hidden_size_choices)\n",
    "        num_choices_layer = len(layer_choices)\n",
    "        \n",
    "        self.arch_params_hidden = torch.nn.Parameter(torch.ones(num_choices_hidden), requires_grad=True)\n",
    "        self.arch_params_layer = torch.nn.Parameter(torch.ones(num_choices_layer), requires_grad=True)\n",
    "        \n",
    "        self.masks = get_channel_mask(hidden_size_choices)\n",
    "        self.flops_choices = get_flops_choices(input_size, hidden_size_choices, num_classes)\n",
    "        self.flops_choices_normalized = torch.FloatTensor(self.flops_choices / np.max(self.flops_choices))\n",
    "        self.layer_choices_tensor = torch.FloatTensor(layer_choices)\n",
    "        \n",
    "        self.fc1 = nn.ModuleList()\n",
    "        for layer_num in layer_choices:\n",
    "            layers = []\n",
    "            layers.append(nn.Linear(input_size, max_hidden_size))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "            for i in range(layer_num-1):\n",
    "                layers.append(nn.Linear(max_hidden_size, max_hidden_size))\n",
    "                layers.append(nn.ReLU(inplace=True))\n",
    "            layers = nn.Sequential(*layers)\n",
    "        self.fc1.append(layers)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(max_hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x, temperature):\n",
    "        out_fc1 = []\n",
    "        for fc1_each in self.fc1:\n",
    "            out_fc1_each = fc1_each(x)\n",
    "            out_fc1.append(out_fc1_each)\n",
    "        out = torch.stack(out_fc1, dim=-1)\n",
    "        \n",
    "        gumbel_weights_layer = gumbel_softmax(self.arch_params_layer, tau=temperature, hard=False)\n",
    "        out = torch.multiply(out, gumbel_weights_layer)\n",
    "        out = torch.sum(out,dim=-1)\n",
    "        # print(self.arch_params)\n",
    "        gumbel_weights_hidden = gumbel_softmax(self.arch_params_hidden, tau=temperature, hard=False)\n",
    "        # print(gumbel_weights)\n",
    "        mask = torch.multiply(self.masks, gumbel_weights_hidden)\n",
    "        mask = torch.sum(mask, dim=-1)\n",
    "        out = torch.multiply(out, mask)\n",
    "        \n",
    "        out = self.fc2(out)\n",
    "        flops_loss = self._get_flops_loss(gumbel_weights_layer, gumbel_weights_hidden)\n",
    "        return out, flops_loss\n",
    "    \n",
    "    def _get_flops_loss(self, gumbel_weights_layer, gumbel_weights_hidden):\n",
    "        layer_factor = torch.matmul(self.layer_choices_tensor, gumbel_weights_layer)\n",
    "        hidden_factor = torch.matmul(self.flops_choices_normalized, gumbel_weights_hidden)\n",
    "        flops = layer_factor * hidden_factor\n",
    "        return flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a0f74cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/1000], Loss: 0.7285, CE_Loss: 0.6919, Flops_Loss: 1.0574, Accuracy: 60.20%, Channel 10, Layer 1\n",
      "Epoch [20/1000], Loss: 0.7186, CE_Loss: 0.6889, Flops_Loss: 0.9864, Accuracy: 60.70%, Channel 10, Layer 1\n",
      "Epoch [30/1000], Loss: 0.7239, CE_Loss: 0.6818, Flops_Loss: 1.1019, Accuracy: 61.10%, Channel 10, Layer 1\n",
      "Epoch [40/1000], Loss: 0.7013, CE_Loss: 0.6684, Flops_Loss: 0.9973, Accuracy: 61.60%, Channel 10, Layer 1\n",
      "Epoch [50/1000], Loss: 0.6771, CE_Loss: 0.6485, Flops_Loss: 0.9343, Accuracy: 62.60%, Channel 10, Layer 1\n",
      "Epoch [60/1000], Loss: 0.6948, CE_Loss: 0.6444, Flops_Loss: 1.1485, Accuracy: 62.90%, Channel 50, Layer 1\n",
      "Epoch [70/1000], Loss: 0.6619, CE_Loss: 0.6284, Flops_Loss: 0.9640, Accuracy: 65.50%, Channel 50, Layer 1\n",
      "Epoch [80/1000], Loss: 0.6574, CE_Loss: 0.6252, Flops_Loss: 0.9474, Accuracy: 65.60%, Channel 50, Layer 1\n",
      "Epoch [90/1000], Loss: 0.6481, CE_Loss: 0.6053, Flops_Loss: 1.0329, Accuracy: 67.40%, Channel 50, Layer 1\n",
      "Epoch [100/1000], Loss: 0.6559, CE_Loss: 0.6026, Flops_Loss: 1.1358, Accuracy: 67.10%, Channel 170, Layer 1\n",
      "Epoch [110/1000], Loss: 0.6321, CE_Loss: 0.5799, Flops_Loss: 1.1014, Accuracy: 70.10%, Channel 170, Layer 1\n",
      "Epoch [120/1000], Loss: 0.6100, CE_Loss: 0.5728, Flops_Loss: 0.9448, Accuracy: 70.90%, Channel 300, Layer 1\n",
      "Epoch [130/1000], Loss: 0.5811, CE_Loss: 0.5410, Flops_Loss: 0.9423, Accuracy: 74.20%, Channel 300, Layer 1\n",
      "Epoch [140/1000], Loss: 0.6174, CE_Loss: 0.5344, Flops_Loss: 1.3640, Accuracy: 74.40%, Channel 300, Layer 1\n",
      "Epoch [150/1000], Loss: 0.5270, CE_Loss: 0.4914, Flops_Loss: 0.8477, Accuracy: 78.90%, Channel 300, Layer 1\n",
      "Epoch [160/1000], Loss: 0.5609, CE_Loss: 0.4860, Flops_Loss: 1.2351, Accuracy: 79.50%, Channel 300, Layer 1\n",
      "Epoch [170/1000], Loss: 0.5002, CE_Loss: 0.4344, Flops_Loss: 1.0923, Accuracy: 84.70%, Channel 300, Layer 1\n",
      "Epoch [180/1000], Loss: 0.4897, CE_Loss: 0.4287, Flops_Loss: 1.0391, Accuracy: 85.40%, Channel 490, Layer 1\n",
      "Epoch [190/1000], Loss: 0.4167, CE_Loss: 0.3779, Flops_Loss: 0.7657, Accuracy: 88.50%, Channel 490, Layer 1\n",
      "Epoch [200/1000], Loss: 0.4145, CE_Loss: 0.3579, Flops_Loss: 0.9243, Accuracy: 89.30%, Channel 490, Layer 1\n",
      "Epoch [210/1000], Loss: 0.3958, CE_Loss: 0.3047, Flops_Loss: 1.2159, Accuracy: 92.60%, Channel 490, Layer 1\n",
      "Epoch [220/1000], Loss: 0.3854, CE_Loss: 0.3222, Flops_Loss: 0.9541, Accuracy: 92.60%, Channel 490, Layer 1\n",
      "Epoch [230/1000], Loss: 0.3293, CE_Loss: 0.2355, Flops_Loss: 1.1735, Accuracy: 95.90%, Channel 490, Layer 1\n",
      "Epoch [240/1000], Loss: 0.3363, CE_Loss: 0.2249, Flops_Loss: 1.3385, Accuracy: 96.00%, Channel 490, Layer 1\n",
      "Epoch [250/1000], Loss: 0.2766, CE_Loss: 0.1906, Flops_Loss: 1.0503, Accuracy: 97.10%, Channel 490, Layer 1\n",
      "Epoch [260/1000], Loss: 0.2343, CE_Loss: 0.1848, Flops_Loss: 0.6796, Accuracy: 97.40%, Channel 490, Layer 1\n",
      "Epoch [270/1000], Loss: 0.2523, CE_Loss: 0.1368, Flops_Loss: 1.2921, Accuracy: 98.60%, Channel 490, Layer 1\n",
      "Epoch [280/1000], Loss: 0.2106, CE_Loss: 0.1465, Flops_Loss: 0.7875, Accuracy: 98.90%, Channel 490, Layer 1\n",
      "Epoch [290/1000], Loss: 0.2439, CE_Loss: 0.1609, Flops_Loss: 0.9905, Accuracy: 99.40%, Channel 490, Layer 1\n",
      "Epoch [300/1000], Loss: 0.1985, CE_Loss: 0.1668, Flops_Loss: 0.4841, Accuracy: 99.60%, Channel 490, Layer 1\n",
      "Epoch [310/1000], Loss: 0.1915, CE_Loss: 0.1205, Flops_Loss: 0.8307, Accuracy: 100.00%, Channel 490, Layer 1\n",
      "Epoch [320/1000], Loss: 0.1723, CE_Loss: 0.0808, Flops_Loss: 0.9957, Accuracy: 100.00%, Channel 490, Layer 1\n",
      "Epoch [330/1000], Loss: 0.1583, CE_Loss: 0.0714, Flops_Loss: 0.9399, Accuracy: 100.00%, Channel 490, Layer 1\n",
      "Epoch [340/1000], Loss: 0.1707, CE_Loss: 0.0383, Flops_Loss: 1.3622, Accuracy: 100.00%, Channel 490, Layer 1\n",
      "Epoch [350/1000], Loss: 0.1906, CE_Loss: 0.0338, Flops_Loss: 1.6020, Accuracy: 100.00%, Channel 490, Layer 1\n",
      "Epoch [360/1000], Loss: 0.1203, CE_Loss: 0.0527, Flops_Loss: 0.7287, Accuracy: 100.00%, Channel 490, Layer 1\n",
      "Epoch [370/1000], Loss: 0.1298, CE_Loss: 0.0414, Flops_Loss: 0.9256, Accuracy: 100.00%, Channel 490, Layer 1\n",
      "Epoch [380/1000], Loss: 0.0837, CE_Loss: 0.0305, Flops_Loss: 0.5632, Accuracy: 100.00%, Channel 490, Layer 1\n",
      "Epoch [390/1000], Loss: 0.1522, CE_Loss: 0.0095, Flops_Loss: 1.4361, Accuracy: 100.00%, Channel 490, Layer 1\n",
      "Epoch [400/1000], Loss: 0.0914, CE_Loss: 0.0430, Flops_Loss: 0.5264, Accuracy: 100.00%, Channel 490, Layer 1\n",
      "Epoch [410/1000], Loss: 0.1225, CE_Loss: 0.0217, Flops_Loss: 1.0304, Accuracy: 100.00%, Channel 490, Layer 1\n",
      "Epoch [420/1000], Loss: 0.0920, CE_Loss: 0.0274, Flops_Loss: 0.6727, Accuracy: 100.00%, Channel 490, Layer 1\n",
      "Epoch [430/1000], Loss: 0.1271, CE_Loss: 0.0141, Flops_Loss: 1.1437, Accuracy: 100.00%, Channel 490, Layer 1\n",
      "Epoch [440/1000], Loss: 0.1644, CE_Loss: 0.1011, Flops_Loss: 0.7348, Accuracy: 100.00%, Channel 490, Layer 1\n",
      "Epoch [450/1000], Loss: 0.1166, CE_Loss: 0.0328, Flops_Loss: 0.8705, Accuracy: 100.00%, Channel 490, Layer 1\n",
      "Epoch [460/1000], Loss: 0.2858, CE_Loss: 0.2932, Flops_Loss: 0.2194, Accuracy: 100.00%, Channel 490, Layer 1\n",
      "Epoch [470/1000], Loss: 0.1039, CE_Loss: 0.0658, Flops_Loss: 0.4470, Accuracy: 100.00%, Channel 490, Layer 1\n",
      "Epoch [480/1000], Loss: 0.1142, CE_Loss: 0.0703, Flops_Loss: 0.5089, Accuracy: 100.00%, Channel 490, Layer 1\n",
      "Epoch [490/1000], Loss: 0.1758, CE_Loss: 0.0059, Flops_Loss: 1.7049, Accuracy: 100.00%, Channel 490, Layer 1\n",
      "Epoch [500/1000], Loss: 0.1582, CE_Loss: 0.0219, Flops_Loss: 1.3848, Accuracy: 100.00%, Channel 490, Layer 1\n",
      "Epoch [510/1000], Loss: 0.0607, CE_Loss: 0.0122, Flops_Loss: 0.4974, Accuracy: 100.00%, Channel 490, Layer 1\n",
      "Epoch [520/1000], Loss: 0.1057, CE_Loss: 0.0348, Flops_Loss: 0.7439, Accuracy: 100.00%, Channel 490, Layer 1\n",
      "Epoch [530/1000], Loss: 0.1240, CE_Loss: 0.0042, Flops_Loss: 1.2018, Accuracy: 100.00%, Channel 490, Layer 1\n",
      "Epoch [540/1000], Loss: 0.1353, CE_Loss: 0.0061, Flops_Loss: 1.2977, Accuracy: 100.00%, Channel 490, Layer 1\n",
      "Epoch [550/1000], Loss: 0.0963, CE_Loss: 0.0007, Flops_Loss: 0.9572, Accuracy: 100.00%, Channel 490, Layer 1\n",
      "Epoch [560/1000], Loss: 0.1701, CE_Loss: 0.1300, Flops_Loss: 0.5309, Accuracy: 100.00%, Channel 490, Layer 1\n",
      "Epoch [570/1000], Loss: 0.1727, CE_Loss: 0.0169, Flops_Loss: 1.5747, Accuracy: 100.00%, Channel 490, Layer 1\n",
      "Epoch [580/1000], Loss: 0.0868, CE_Loss: 0.0226, Flops_Loss: 0.6649, Accuracy: 100.00%, Channel 490, Layer 1\n",
      "Epoch [590/1000], Loss: 0.0972, CE_Loss: 0.0006, Flops_Loss: 0.9665, Accuracy: 100.00%, Channel 490, Layer 1\n",
      "Epoch [600/1000], Loss: 0.1533, CE_Loss: 0.1410, Flops_Loss: 0.2636, Accuracy: 100.00%, Channel 490, Layer 1\n",
      "Epoch [610/1000], Loss: 0.1417, CE_Loss: 0.0003, Flops_Loss: 1.4144, Accuracy: 100.00%, Channel 490, Layer 1\n",
      "Epoch [620/1000], Loss: 0.0733, CE_Loss: 0.0121, Flops_Loss: 0.6240, Accuracy: 100.00%, Channel 500, Layer 1\n",
      "Epoch [630/1000], Loss: 0.0664, CE_Loss: 0.0131, Flops_Loss: 0.5457, Accuracy: 100.00%, Channel 500, Layer 1\n",
      "Epoch [640/1000], Loss: 0.1466, CE_Loss: 0.0019, Flops_Loss: 1.4490, Accuracy: 100.00%, Channel 490, Layer 1\n",
      "Epoch [650/1000], Loss: 0.0770, CE_Loss: 0.0563, Flops_Loss: 0.2633, Accuracy: 100.00%, Channel 490, Layer 1\n",
      "Epoch [660/1000], Loss: 0.0946, CE_Loss: 0.0001, Flops_Loss: 0.9446, Accuracy: 100.00%, Channel 490, Layer 1\n",
      "Epoch [670/1000], Loss: 0.0666, CE_Loss: 0.0423, Flops_Loss: 0.2852, Accuracy: 100.00%, Channel 490, Layer 1\n",
      "Epoch [680/1000], Loss: 0.0770, CE_Loss: 0.0008, Flops_Loss: 0.7631, Accuracy: 100.00%, Channel 490, Layer 1\n",
      "Epoch [690/1000], Loss: 0.0853, CE_Loss: 0.0002, Flops_Loss: 0.8509, Accuracy: 100.00%, Channel 490, Layer 1\n",
      "Epoch [700/1000], Loss: 0.0773, CE_Loss: 0.0066, Flops_Loss: 0.7130, Accuracy: 100.00%, Channel 490, Layer 1\n",
      "Epoch [710/1000], Loss: 0.0697, CE_Loss: 0.0004, Flops_Loss: 0.6937, Accuracy: 100.00%, Channel 490, Layer 1\n",
      "Epoch [720/1000], Loss: 0.0820, CE_Loss: 0.0001, Flops_Loss: 0.8193, Accuracy: 100.00%, Channel 500, Layer 1\n",
      "Epoch [730/1000], Loss: 0.0830, CE_Loss: 0.0601, Flops_Loss: 0.2892, Accuracy: 100.00%, Channel 500, Layer 1\n",
      "Epoch [740/1000], Loss: 0.0862, CE_Loss: 0.0001, Flops_Loss: 0.8617, Accuracy: 100.00%, Channel 500, Layer 1\n",
      "Epoch [750/1000], Loss: 0.1576, CE_Loss: 0.0001, Flops_Loss: 1.5749, Accuracy: 100.00%, Channel 500, Layer 1\n",
      "Epoch [760/1000], Loss: 0.2702, CE_Loss: 0.0000, Flops_Loss: 2.7020, Accuracy: 100.00%, Channel 500, Layer 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [770/1000], Loss: 0.0508, CE_Loss: 0.0311, Flops_Loss: 0.2280, Accuracy: 100.00%, Channel 500, Layer 1\n",
      "Epoch [780/1000], Loss: 0.2229, CE_Loss: 0.0002, Flops_Loss: 2.2271, Accuracy: 100.00%, Channel 500, Layer 1\n",
      "Epoch [790/1000], Loss: 0.0534, CE_Loss: 0.0009, Flops_Loss: 0.5253, Accuracy: 100.00%, Channel 500, Layer 1\n",
      "Epoch [800/1000], Loss: 0.0631, CE_Loss: 0.0004, Flops_Loss: 0.6270, Accuracy: 100.00%, Channel 500, Layer 1\n",
      "Epoch [810/1000], Loss: 0.5289, CE_Loss: 0.5843, Flops_Loss: 0.0307, Accuracy: 100.00%, Channel 500, Layer 1\n",
      "Epoch [820/1000], Loss: 0.0672, CE_Loss: 0.0075, Flops_Loss: 0.6043, Accuracy: 100.00%, Channel 480, Layer 1\n",
      "Epoch [830/1000], Loss: 0.0684, CE_Loss: 0.0339, Flops_Loss: 0.3787, Accuracy: 100.00%, Channel 480, Layer 1\n",
      "Epoch [840/1000], Loss: 0.0940, CE_Loss: 0.0000, Flops_Loss: 0.9394, Accuracy: 100.00%, Channel 480, Layer 1\n",
      "Epoch [850/1000], Loss: 0.1205, CE_Loss: 0.1226, Flops_Loss: 0.1010, Accuracy: 100.00%, Channel 480, Layer 1\n",
      "Epoch [860/1000], Loss: 0.2515, CE_Loss: 0.2738, Flops_Loss: 0.0508, Accuracy: 100.00%, Channel 480, Layer 1\n",
      "Epoch [870/1000], Loss: 0.0368, CE_Loss: 0.0061, Flops_Loss: 0.3133, Accuracy: 100.00%, Channel 480, Layer 1\n",
      "Epoch [880/1000], Loss: 0.0790, CE_Loss: 0.0003, Flops_Loss: 0.7879, Accuracy: 100.00%, Channel 480, Layer 1\n",
      "Epoch [890/1000], Loss: 0.0361, CE_Loss: 0.0143, Flops_Loss: 0.2323, Accuracy: 100.00%, Channel 480, Layer 1\n",
      "Epoch [900/1000], Loss: 0.1544, CE_Loss: 0.1542, Flops_Loss: 0.1558, Accuracy: 100.00%, Channel 480, Layer 1\n",
      "Epoch [910/1000], Loss: 0.0922, CE_Loss: 0.0082, Flops_Loss: 0.8482, Accuracy: 100.00%, Channel 480, Layer 1\n",
      "Epoch [920/1000], Loss: 0.0705, CE_Loss: 0.0637, Flops_Loss: 0.1314, Accuracy: 100.00%, Channel 480, Layer 1\n",
      "Epoch [930/1000], Loss: 0.0427, CE_Loss: 0.0015, Flops_Loss: 0.4144, Accuracy: 100.00%, Channel 480, Layer 1\n",
      "Epoch [940/1000], Loss: 0.0524, CE_Loss: 0.0414, Flops_Loss: 0.1513, Accuracy: 100.00%, Channel 480, Layer 1\n",
      "Epoch [950/1000], Loss: 0.0594, CE_Loss: 0.0256, Flops_Loss: 0.3636, Accuracy: 100.00%, Channel 480, Layer 1\n",
      "Epoch [960/1000], Loss: 0.0328, CE_Loss: 0.0050, Flops_Loss: 0.2828, Accuracy: 100.00%, Channel 480, Layer 1\n",
      "Epoch [970/1000], Loss: 0.1194, CE_Loss: 0.0014, Flops_Loss: 1.1818, Accuracy: 100.00%, Channel 480, Layer 1\n",
      "Epoch [980/1000], Loss: 0.0438, CE_Loss: 0.0014, Flops_Loss: 0.4254, Accuracy: 100.00%, Channel 480, Layer 1\n",
      "Epoch [990/1000], Loss: 0.0608, CE_Loss: 0.0002, Flops_Loss: 0.6060, Accuracy: 100.00%, Channel 480, Layer 1\n",
      "Epoch [1000/1000], Loss: 0.0643, CE_Loss: 0.0579, Flops_Loss: 0.1212, Accuracy: 100.00%, Channel 480, Layer 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Initialize the neural network\n",
    "in_dim = x_train.shape[1]\n",
    "hidden_size_choices = list(range(10,1000,10))\n",
    "layer_choices = [1,2,3]\n",
    "\n",
    "num_classes =  2\n",
    "\n",
    "net = SuperNet_CS_LS(in_dim, hidden_size_choices, layer_choices, num_classes)\n",
    "\n",
    "# Set up the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "net_weight_lr = 0.0001\n",
    "arch_lr = 0.001\n",
    "\n",
    "optimizer_net = optim.Adam([p for name, p in net.named_parameters() if 'arch' not in name], lr=net_weight_lr)\n",
    "optimizer_arch = optim.Adam([p for name, p in net.named_parameters() if 'arch' in name], lr=arch_lr)\n",
    "\n",
    "# Search Epoch\n",
    "num_epochs = 1000\n",
    "\n",
    "# Iteartively optimize architecture parameters and network weights every 'search_freq' epochs\n",
    "search_freq = 10\n",
    "\n",
    "# The factor to balance performance (CE Loss or MSE Loss) and FLOPs\n",
    "# Larger flops_balance_factor leads to a faster network with worse performance\n",
    "flops_balance_factor = 0.1\n",
    "\n",
    "# The tempreature is decayed by 'temp_anneal_factor' every 'temp_anneal_freq' epochs\n",
    "# Larger temp leads to a gumbel weight that is more close to 1-hot distribution  \n",
    "temp = 5\n",
    "temp_anneal_factor = 0.95\n",
    "temp_anneal_freq = num_epochs/100 # The temperatur will decay 25 times during the search\n",
    "\n",
    "warmup = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    if (epoch % temp_anneal_freq == 0) and(epoch>=warmup):\n",
    "        temp = temp * temp_anneal_factor\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs, flops_loss = net(x_train, temp)\n",
    "    CE_loss = criterion(outputs, y_train)\n",
    "    loss = (1 - flops_balance_factor) * CE_loss + flops_balance_factor * flops_loss\n",
    "    \n",
    "    if (int(epoch/search_freq)%2==0) or (epoch<=warmup):\n",
    "        optimizer_net.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_net.step()\n",
    "    else:\n",
    "        optimizer_arch.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_arch.step()\n",
    "    \n",
    "    selected_channel_id = np.argmax(net.arch_params_hidden.data.numpy())\n",
    "    selected_channel = hidden_size_choices[selected_channel_id]\n",
    "    \n",
    "    selected_layer_id = np.argmax(net.arch_params_layer.data.numpy())\n",
    "    selected_layer = layer_choices[selected_layer_id]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total = y_train.size(0)\n",
    "        correct = (predicted == y_train).sum().item()\n",
    "        accuracy = correct / total\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            \n",
    "            print('Epoch [{}/{}], Loss: {:.4f}, CE_Loss: {:.4f}, Flops_Loss: {:.4f}, Accuracy: {:.2f}%, Channel {}, Layer {}'\n",
    "                  .format(epoch+1, num_epochs, loss.item(), CE_loss.item(),\n",
    "                          flops_loss.item(), accuracy * 100, selected_channel, selected_layer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
